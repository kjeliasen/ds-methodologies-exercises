{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark API Mini Exercises\n",
    "\n",
    "### Contents\n",
    "- [Section I](#Section-I)\n",
    "- [Section II](#Section-II)\n",
    "- [Section III](#Section-III)\n",
    "- [Section IV](#Section-IV)\n",
    "- [Section V](#Section-V)\n",
    "- [Section VI](#Section-VI)\n",
    "- [Section VII](#Section-VII)\n",
    "- [Section VIII](#Section-VIII)\n",
    "- [Section IX](#Section-IX)\n",
    "- [Section X](#Section-X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the code below to create a pandas dataframe with 20 rows and 3 columns:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(13)\n",
    "\n",
    "pandas_dataframe = pd.DataFrame(\n",
    "    {\n",
    "        \"n\": np.random.randn(20),\n",
    "        \"group\": np.random.choice(list(\"xyz\"), 20),\n",
    "        \"abool\": np.random.choice([True, False], 20),\n",
    "    }\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Spark Dataframe Basics\n",
    "\n",
    "    1. Use the starter code above to create a pandas dataframe.\n",
    "    1. Convert the pandas dataframe to a spark dataframe. From this point\n",
    "       forward, do all of your work with the spark dataframe, not the pandas\n",
    "       dataframe.\n",
    "    1. Show the first 3 rows of the dataframe.\n",
    "    1. Show the first 7 rows of the dataframe.\n",
    "    1. View a summary of the data using `.describe`.\n",
    "    1. Use `.select` to create a new dataframe with just the `n` and `abool`\n",
    "       columns. View the first 5 rows of this dataframe.\n",
    "    1. Use `.select` to create a new dataframe with just the `group` and `abool`\n",
    "       columns. View the first 5 rows of this dataframe.\n",
    "    1. Use `.select` to create a new dataframe with the `group` column and the\n",
    "       `abool` column renamed to `a_boolean_value`. Show the first 3 rows of\n",
    "       this dataframe.\n",
    "    1. Use `.select` to create a new dataframe with the `group` column and the\n",
    "       `n` column renamed to `a_numeric_value`. Show the first 6 rows of this\n",
    "       dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I.A Use the starter code above to create a pandas dataframe.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(13)\n",
    "\n",
    "pandas_dataframe = pd.DataFrame(\n",
    "    {\n",
    "        \"n\": np.random.randn(20),\n",
    "        \"group\": np.random.choice(list(\"xyz\"), 20),\n",
    "        \"abool\": np.random.choice([True, False], 20),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I.B Convert the pandas dataframe to a spark dataframe. From this point forward, \n",
    "# do all of your work with the spark dataframe, not the pandas dataframe.\n",
    "\n",
    "dfi = spark.createDataFrame(pandas_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "|  -0.712390662050588|    z|false|\n",
      "|   0.753766378659703|    x|false|\n",
      "|-0.04450307833805...|    z|false|\n",
      "+--------------------+-----+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# I.C Show the first 3 rows of the dataframe.\n",
    "\n",
    "dfi.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "|  -0.712390662050588|    z|false|\n",
      "|   0.753766378659703|    x|false|\n",
      "|-0.04450307833805...|    z|false|\n",
      "| 0.45181233874578974|    y|false|\n",
      "|  1.3451017084510097|    z|false|\n",
      "|  0.5323378882945463|    y|false|\n",
      "|  1.3501878997225267|    z|false|\n",
      "+--------------------+-----+-----+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# I.D Show the first 7 rows of the dataframe.\n",
    "\n",
    "dfi.show(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-----+\n",
      "|summary|                  n|group|\n",
      "+-------+-------------------+-----+\n",
      "|  count|                 20|   20|\n",
      "|   mean|0.36640264498852165| null|\n",
      "| stddev| 0.8905322898155364| null|\n",
      "|    min| -1.261605945319069|    x|\n",
      "|    max| 2.1503829673811126|    z|\n",
      "+-------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# I.E View a summary of the data using .describe.\n",
    "\n",
    "dfi.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                   n|abool|\n",
      "+--------------------+-----+\n",
      "|  -0.712390662050588|false|\n",
      "|   0.753766378659703|false|\n",
      "|-0.04450307833805...|false|\n",
      "| 0.45181233874578974|false|\n",
      "|  1.3451017084510097|false|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# I.F Use .select to create a new dataframe with just the n and abool columns. \n",
    "#     View the first 5 rows of this dataframe.\n",
    "\n",
    "dfi_nabool = dfi.select('n','abool')\n",
    "dfi_nabool.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|group|abool|\n",
      "+-----+-----+\n",
      "|    z|false|\n",
      "|    x|false|\n",
      "|    z|false|\n",
      "|    y|false|\n",
      "|    z|false|\n",
      "+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# I.G Use .select to create a new dataframe with just the group and abool columns. \n",
    "#     View the first 5 rows of this dataframe.\n",
    "\n",
    "dfi_groupabool = dfi.select('group','abool')\n",
    "dfi_groupabool.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+\n",
      "|group|a_boolean_value|\n",
      "+-----+---------------+\n",
      "|    z|          false|\n",
      "|    x|          false|\n",
      "|    z|          false|\n",
      "+-----+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# I.H Use .select to create a new dataframe with the group column and the abool \n",
    "#     column renamed to a_boolean_value. Show the first 3 rows of this dataframe.\n",
    "\n",
    "dfi_groupaboolean = dfi.select('group',dfi.abool.alias('a_boolean_value'))\n",
    "dfi_groupaboolean.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|group|     a_numeric_value|\n",
      "+-----+--------------------+\n",
      "|    z|  -0.712390662050588|\n",
      "|    x|   0.753766378659703|\n",
      "|    z|-0.04450307833805...|\n",
      "|    y| 0.45181233874578974|\n",
      "|    z|  1.3451017084510097|\n",
      "|    y|  0.5323378882945463|\n",
      "+-----+--------------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# I.J Use .select to create a new dataframe with the group column and the n column \n",
    "#     renamed to a_numeric_value. Show the first 6 rows of this dataframe.\n",
    "\n",
    "dfi_groupnum = dfi.select('group',dfi.n.alias('a_numeric_value'))\n",
    "dfi_groupnum.show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Column Manipulation\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe. Store the\n",
    "       spark dataframe in a varaible named `df`\n",
    "\n",
    "    1. Use `.select` to add 4 to the `n` column. Show the results.\n",
    "\n",
    "    1. Subtract 5 from the `n` column and view the results.\n",
    "\n",
    "    1. Multiply the `n` column by 2. View the results along with the original\n",
    "       numbers.\n",
    "\n",
    "    1. Add a new column named `n2` that is the `n` value multiplied by -1. Show\n",
    "       the first 4 rows of your dataframe. You should see the original `n` value\n",
    "       as well as `n2`.\n",
    "\n",
    "    1. Add a new column named `n3` that is the n value squared. Show the first 5\n",
    "       rows of your dataframe. You should see both `n`, `n2`, and `n3`.\n",
    "\n",
    "    1. What happens when you run the code below?\n",
    "\n",
    "        ```python\n",
    "        df.group + df.abool\n",
    "        ```\n",
    "\n",
    "    1. What happens when you run the code below? What is the difference between\n",
    "       this and the previous code sample?\n",
    "\n",
    "        ```python\n",
    "        df.select(df.group + df.abool)\n",
    "        ```\n",
    "\n",
    "    1. Try adding various other columns together. What are the results of\n",
    "       combining the different data types?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# II.A. Use the starter code above to re-create a spark dataframe. \n",
    "#     Store the spark dataframe in a varaible named df\n",
    "\n",
    "dfii = spark.createDataFrame(pandas_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+------------------+\n",
      "|                   n|group|abool|          n_plus_4|\n",
      "+--------------------+-----+-----+------------------+\n",
      "|  -0.712390662050588|    z|false|3.2876093379494122|\n",
      "|   0.753766378659703|    x|false| 4.753766378659703|\n",
      "|-0.04450307833805...|    z|false|3.9554969216619464|\n",
      "| 0.45181233874578974|    y|false|  4.45181233874579|\n",
      "|  1.3451017084510097|    z|false|5.3451017084510095|\n",
      "+--------------------+-----+-----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# II.B Use .select to add 4 to the n column. Show the results.\n",
    "\n",
    "n_plus_4 = (dfii.n+4).alias('n_plus_4')\n",
    "dfii.select('*', n_plus_4).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+-------------------+\n",
      "|                   n|group|abool|           n_less_5|\n",
      "+--------------------+-----+-----+-------------------+\n",
      "|  -0.712390662050588|    z|false| -5.712390662050588|\n",
      "|   0.753766378659703|    x|false| -4.246233621340297|\n",
      "|-0.04450307833805...|    z|false| -5.044503078338053|\n",
      "| 0.45181233874578974|    y|false|  -4.54818766125421|\n",
      "|  1.3451017084510097|    z|false|-3.6548982915489905|\n",
      "+--------------------+-----+-----+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# II.C Subtract 5 from the n column and view the results.\n",
    "\n",
    "n_less_5 = (dfii.n - 5).alias('n_less_5')\n",
    "dfii.select('*', n_less_5).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+--------------------+\n",
      "|                   n|group|abool|           n_times_2|\n",
      "+--------------------+-----+-----+--------------------+\n",
      "|  -0.712390662050588|    z|false|  -1.424781324101176|\n",
      "|   0.753766378659703|    x|false|   1.507532757319406|\n",
      "|-0.04450307833805...|    z|false|-0.08900615667610691|\n",
      "| 0.45181233874578974|    y|false|  0.9036246774915795|\n",
      "|  1.3451017084510097|    z|false|  2.6902034169020195|\n",
      "+--------------------+-----+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# II.D Multiply the n column by 2. View the results along with the original numbers.\n",
    "\n",
    "n_times_2 = (dfii.n * 2).alias('n_times_2')\n",
    "dfii.select('*', n_times_2).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+--------------------+\n",
      "|                   n|group|abool|                  n2|\n",
      "+--------------------+-----+-----+--------------------+\n",
      "|  -0.712390662050588|    z|false|   0.712390662050588|\n",
      "|   0.753766378659703|    x|false|  -0.753766378659703|\n",
      "|-0.04450307833805...|    z|false|0.044503078338053455|\n",
      "| 0.45181233874578974|    y|false|-0.45181233874578974|\n",
      "+--------------------+-----+-----+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# II.E Add a new column named n2 that is the n value multiplied by -1. \n",
    "#     Show the first 4 rows of your dataframe. \n",
    "#     You should see the original n value as well as n2.\n",
    "\n",
    "n2 = (dfii.n * -1).alias('n2')\n",
    "dfii = dfii.select('*', n2)\n",
    "dfii.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+--------------------+--------------------+\n",
      "|                   n|group|abool|                  n2|                  n3|\n",
      "+--------------------+-----+-----+--------------------+--------------------+\n",
      "|  -0.712390662050588|    z|false|   0.712390662050588|   0.507500455376875|\n",
      "|   0.753766378659703|    x|false|  -0.753766378659703|  0.5681637535977627|\n",
      "|-0.04450307833805...|    z|false|0.044503078338053455|0.001980523981562...|\n",
      "| 0.45181233874578974|    y|false|-0.45181233874578974| 0.20413438944294027|\n",
      "|  1.3451017084510097|    z|false| -1.3451017084510097|  1.8092986060778251|\n",
      "+--------------------+-----+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# II.F Add a new column named n3 that is the n value squared. \n",
    "#     Show the first 5 rows of your dataframe. You should see both n, n2, and n3.\n",
    "\n",
    "n3 = (dfii.n ** 2).alias('n3')\n",
    "dfii = dfii.select('*', n3)\n",
    "dfii.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'(group + abool)'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# II.G What happens when you run the code below?\n",
    "#     '''df.group + df.abool'''\n",
    "\n",
    "dfii.group + dfii.abool\n",
    "\n",
    "# OUTPUT: Column<b'(group + abool)'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# II.H What happens when you run the code below? What is the difference between \n",
    "#     this and the previous code sample?\n",
    "#     '''df.select(df.group + df.abool)'''\n",
    "\n",
    "# dfii.select(dfii.group + dfii.abool)\n",
    "\n",
    "# OUTPUT: ERROR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output II.H\n",
    "[See error](#Example-II.H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# II.I Try adding various other columns together. What are the results of \n",
    "#     combining the different data types?\n",
    "\n",
    "# dfii.select(dfii.n + dfii.abool).show(5)\n",
    "\n",
    "# OUTPUT: Error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output II.I\n",
    "[See error](#Example-II.I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            (n + n3)|\n",
      "+--------------------+\n",
      "|-0.20489020667371294|\n",
      "|  1.3219301322574657|\n",
      "|-0.04252255435649...|\n",
      "|    0.65594672818873|\n",
      "|   3.154400314528835|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfii.select(dfii.n + dfii.n3).show(5)\n",
    "\n",
    "# OUTPUT: Mathemagics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section III"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Spark SQL\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe.\n",
    "    1. Turn your dataframe into a table that can be queried with spark SQL. Name\n",
    "       the table `my_df`. Answer the rest of the questions in this section with\n",
    "       a spark sql query (`spark.sql`) against `my_df`. After each step, view\n",
    "       the first 7 records from the dataframe.\n",
    "    1. Write a query that shows all of the columns from your dataframe.\n",
    "    1. Write a query that shows just the `n` and `abool` columns from the\n",
    "       dataframe.\n",
    "    1. Write a query that shows just the `n` and `group` columns. Rename the\n",
    "       `group` column to `g`.\n",
    "    1. Write a query that selects `n`, and creates two new columns: `n2`, the\n",
    "       original `n` values halved, and `n3`: the original n values minus 1.\n",
    "    1. What happens if you make a SQL syntax error in your query?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# III.A Use the starter code above to re-create a spark dataframe.\n",
    "\n",
    "dfiii = spark.createDataFrame(pandas_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "|  -0.712390662050588|    z|false|\n",
      "|   0.753766378659703|    x|false|\n",
      "|-0.04450307833805...|    z|false|\n",
      "| 0.45181233874578974|    y|false|\n",
      "|  1.3451017084510097|    z|false|\n",
      "|  0.5323378882945463|    y|false|\n",
      "|  1.3501878997225267|    z|false|\n",
      "+--------------------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# III.B Turn your dataframe into a table that can be queried with spark SQL. \n",
    "#     Name the table my_df. Answer the rest of the questions in this section \n",
    "#     with a spark sql query (spark.sql) against my_df. After each step, view \n",
    "#     the first 7 records from the dataframe.\n",
    "\n",
    "my_df = dfiii\n",
    "# display(my_df.show())\n",
    "my_df.createOrReplaceTempView(\"my_df\")\n",
    "\n",
    "spark.sql('''\n",
    "SELECT * FROM my_df LIMIT 7\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "|  -0.712390662050588|    z|false|\n",
      "|   0.753766378659703|    x|false|\n",
      "|-0.04450307833805...|    z|false|\n",
      "| 0.45181233874578974|    y|false|\n",
      "|  1.3451017084510097|    z|false|\n",
      "+--------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# III.C Write a query that shows all of the columns from your dataframe.\n",
    "\n",
    "spark.sql('''\n",
    "SELECT * FROM my_df\n",
    "''').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                   n|abool|\n",
      "+--------------------+-----+\n",
      "|  -0.712390662050588|false|\n",
      "|   0.753766378659703|false|\n",
      "|-0.04450307833805...|false|\n",
      "| 0.45181233874578974|false|\n",
      "|  1.3451017084510097|false|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# III.D Write a query that shows just the n and abool columns from the dataframe.\n",
    "\n",
    "spark.sql('''\n",
    "SELECT n, abool FROM my_df\n",
    "''').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|                   n|  g|\n",
      "+--------------------+---+\n",
      "|  -0.712390662050588|  z|\n",
      "|   0.753766378659703|  x|\n",
      "|-0.04450307833805...|  z|\n",
      "| 0.45181233874578974|  y|\n",
      "|  1.3451017084510097|  z|\n",
      "+--------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# III.E Write a query that shows just the n and group columns. \n",
    "#     Rename the group column to g.\n",
    "\n",
    "spark.sql('''\n",
    "SELECT n, group as g FROM my_df\n",
    "''').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                   n|                  n2|                  n3|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|  -0.712390662050588|  -0.356195331025294|  -1.712390662050588|\n",
      "|   0.753766378659703|  0.3768831893298515|-0.24623362134029703|\n",
      "|-0.04450307833805...|-0.02225153916902...| -1.0445030783380536|\n",
      "| 0.45181233874578974| 0.22590616937289487| -0.5481876612542103|\n",
      "|  1.3451017084510097|  0.6725508542255049| 0.34510170845100974|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# III.F Write a query that selects n, and creates two new columns: \n",
    "#     n2, the original n values halved, and n3: the original n values minus 1.\n",
    "\n",
    "spark.sql('''\n",
    "SELECT n, (n/2) as n2, (n-1) as n3 FROM my_df\n",
    "''').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# III.G What happens if you make a SQL syntax error in your query?\n",
    "\n",
    "# spark.sql('''\n",
    "# SELECT l, n, (n/2) as n2, (n-1) as n3 FROM my_df\n",
    "# ''').show(5)\n",
    "\n",
    "# OUTPUT: ERROR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output III.G\n",
    "[see error](#Example-III.G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section IV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type casting\n",
    "\n",
    "4. Type casting\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe.\n",
    "\n",
    "    1. Use `.printSchema` to view the datatypes in your dataframe.\n",
    "\n",
    "    1. Use `.dtypes` to view the datatypes in your dataframe.\n",
    "\n",
    "    1. What is the difference between the two code samples below?\n",
    "\n",
    "        ```python\n",
    "        df.abool.cast('int')\n",
    "        ```\n",
    "\n",
    "        ```python\n",
    "        df.select(df.abool.cast('int')).show()\n",
    "        ```\n",
    "\n",
    "    1. Use `.select` and `.cast` to convert the `abool` column to an integer\n",
    "       type. View the results.\n",
    "    1. Convert the `group` column to a integer data type and view the results.\n",
    "       What happens?\n",
    "    1. Convert the `n` column to a integer data type and view the results. What\n",
    "       happens?\n",
    "    1. Convert the `abool` column to a string data type and view the results.\n",
    "       What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Built-in Functions\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe.\n",
    "    1. Import the necessary functions from `pyspark.sql.functions`\n",
    "    1. Find the highest `n` value.\n",
    "    1. Find the lowest `n` value.\n",
    "    1. Find the average `n` value.\n",
    "    1. Use `concat` to change the `group` column to say, e.g. \"Group: x\" or\n",
    "       \"Group: y\"\n",
    "    1. Use `concat` to combine the `n` and `group` columns to produce results\n",
    "       that look like this: \"x: -1.432\" or \"z: 2.352\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section VI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Filter / Where\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe.\n",
    "    1. Use `.filter` or `.where` to select just the rows where the group is `y`\n",
    "       and view the results.\n",
    "    1. Select just the columns where the `abool` column is false and view the\n",
    "       results.\n",
    "    1. Find the columns where the `group` column is *not* `y`.\n",
    "    1. Find the columns where `n` is positive.\n",
    "    1. Find the columns where `abool` is true and the `group` column is `z`.\n",
    "    1. Find the columns where `abool` is true or the `group` column is `z`.\n",
    "    1. Find the columns where `abool` is false and `n` is less than 1\n",
    "    1. Find the columns where `abool` is false or `n` is less than 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section VII"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. When / Otherwise\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe.\n",
    "    1. Use `when` and `.otherwise` to create a column that contains the text \"It\n",
    "       is true\" when `abool` is true and \"It is false\"\" when `abool` is false.\n",
    "    1. Create a column that contains 0 if n is less than 0, otherwise, the\n",
    "       original n value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section VII"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Sorting\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe.\n",
    "    1. Sort by the `n` value.\n",
    "    1. Sort by the `group` value, both ascending and descending.\n",
    "    1. Sort by the group value first, then, within each group, sort by `n`\n",
    "       value.\n",
    "    1. Sort by `abool`, `group`, and `n`. Does it matter in what order you\n",
    "       specify the columns when sorting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section IX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Aggregating\n",
    "\n",
    "    1. What is the average `n` value for each group in the `group` column?\n",
    "    1. What is the maximum `n` value for each group in the `group` column?\n",
    "    1. What is the minimum `n` value by `abool`?\n",
    "    1. What is the average `n` value for each unique combination of the `group`\n",
    "       and `abool` column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example II.H\n",
    "[Return](#Output-II.H)\n",
    "\n",
    "\n",
    "<code>---------------------------------------------------------------------------\n",
    "Py4JJavaError                             Traceback (most recent call last)\n",
    "/usr/local/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py in deco(*a, **kw)\n",
    "     62         try:\n",
    "---> 63             return f(*a, **kw)\n",
    "     64         except py4j.protocol.Py4JJavaError as e:\n",
    "\n",
    "/usr/local/anaconda3/lib/python3.7/site-packages/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)\n",
    "    327                     \"An error occurred while calling {0}{1}{2}.\\n\".\n",
    "--> 328                     format(target_id, \".\", name), value)\n",
    "    329             else:\n",
    "\n",
    "Py4JJavaError: An error occurred while calling o123.select.\n",
    ": org.apache.spark.sql.AnalysisException: cannot resolve '(CAST(`group` AS DOUBLE) + `abool`)' due to data type mismatch: differing types in '(CAST(`group` AS DOUBLE) + `abool`)' (double and boolean).;;\n",
    "'Project [(cast(group#179 as double) + abool#180) AS (group + abool)#284]\n",
    "+- Project [n#178, group#179, abool#180, n2#238, POWER(n#178, cast(2 as double)) AS n3#256]\n",
    "   +- Project [n#178, group#179, abool#180, (n#178 * cast(-1 as double)) AS n2#238]\n",
    "      +- LogicalRDD [n#178, group#179, abool#180], false\n",
    "\n",
    "\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:116)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)\n",
    "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:280)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:329)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:278)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n",
    "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n",
    "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
    "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
    "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
    "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
    "\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
    "\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n",
    "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n",
    "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n",
    "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n",
    "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n",
    "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n",
    "\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3412)\n",
    "\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1340)\n",
    "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
    "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
    "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
    "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
    "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
    "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
    "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
    "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
    "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
    "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
    "\tat java.lang.Thread.run(Thread.java:748)\n",
    "\n",
    "\n",
    "During handling of the above exception, another exception occurred:\n",
    "\n",
    "AnalysisException                         Traceback (most recent call last)\n",
    "<ipython-input-21-435d2c8b5834> in <module>\n",
    "      2 #  df.select(df.group + df.abool)\n",
    "      3 \n",
    "----> 4 dfii.select(dfii.group + dfii.abool)\n",
    "      5 \n",
    "      6 # OUTPUT: ERROR\n",
    "\n",
    "/usr/local/anaconda3/lib/python3.7/site-packages/pyspark/sql/dataframe.py in select(self, *cols)\n",
    "   1319         [Row(name=u'Alice', age=12), Row(name=u'Bob', age=15)]\n",
    "   1320         \"\"\"\n",
    "-> 1321         jdf = self._jdf.select(self._jcols(*cols))\n",
    "   1322         return DataFrame(jdf, self.sql_ctx)\n",
    "   1323 \n",
    "\n",
    "/usr/local/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py in __call__(self, *args)\n",
    "   1255         answer = self.gateway_client.send_command(command)\n",
    "   1256         return_value = get_return_value(\n",
    "-> 1257             answer, self.gateway_client, self.target_id, self.name)\n",
    "   1258 \n",
    "   1259         for temp_arg in temp_args:\n",
    "\n",
    "/usr/local/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py in deco(*a, **kw)\n",
    "     67                                              e.java_exception.getStackTrace()))\n",
    "     68             if s.startswith('org.apache.spark.sql.AnalysisException: '):\n",
    "---> 69                 raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
    "     70             if s.startswith('org.apache.spark.sql.catalyst.analysis'):\n",
    "     71                 raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
    "\n",
    "AnalysisException: \"cannot resolve '(CAST(`group` AS DOUBLE) + `abool`)' due to data type mismatch: differing types in '(CAST(`group` AS DOUBLE) + `abool`)' (double and boolean).;;\\n'Project [(cast(group#179 as double) + abool#180) AS (group + abool)#284]\\n+- Project [n#178, group#179, abool#180, n2#238, POWER(n#178, cast(2 as double)) AS n3#256]\\n   +- Project [n#178, group#179, abool#180, (n#178 * cast(-1 as double)) AS n2#238]\\n      +- LogicalRDD [n#178, group#179, abool#180], false\\n\"</code>\n",
    "\n",
    "[Return](#Output-II.H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example II.I\n",
    "[Return](#Output-II.I)\n",
    "\n",
    "<code>---------------------------------------------------------------------------\n",
    "Py4JJavaError                             Traceback (most recent call last)\n",
    "/usr/local/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py in deco(*a, **kw)\n",
    "     62         try:\n",
    "---> 63             return f(*a, **kw)\n",
    "     64         except py4j.protocol.Py4JJavaError as e:\n",
    "\n",
    "/usr/local/anaconda3/lib/python3.7/site-packages/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)\n",
    "    327                     \"An error occurred while calling {0}{1}{2}.\\n\".\n",
    "--> 328                     format(target_id, \".\", name), value)\n",
    "    329             else:\n",
    "\n",
    "Py4JJavaError: An error occurred while calling o123.select.\n",
    ": org.apache.spark.sql.AnalysisException: cannot resolve '(`n` + `abool`)' due to data type mismatch: differing types in '(`n` + `abool`)' (double and boolean).;;\n",
    "'Project [(n#178 + abool#180) AS (n + abool)#285]\n",
    "+- Project [n#178, group#179, abool#180, n2#238, POWER(n#178, cast(2 as double)) AS n3#256]\n",
    "   +- Project [n#178, group#179, abool#180, (n#178 * cast(-1 as double)) AS n2#238]\n",
    "      +- LogicalRDD [n#178, group#179, abool#180], false\n",
    "\n",
    "\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:116)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)\n",
    "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:280)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:329)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:278)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n",
    "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n",
    "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
    "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
    "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
    "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
    "\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
    "\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n",
    "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n",
    "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n",
    "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n",
    "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n",
    "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n",
    "\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3412)\n",
    "\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1340)\n",
    "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
    "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
    "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
    "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
    "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
    "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
    "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
    "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
    "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
    "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
    "\tat java.lang.Thread.run(Thread.java:748)\n",
    "\n",
    "\n",
    "During handling of the above exception, another exception occurred:\n",
    "\n",
    "AnalysisException                         Traceback (most recent call last)\n",
    "<ipython-input-23-fad6529b8e19> in <module>\n",
    "      1 # I Try adding various other columns together. What are the results of combining the different data types?\n",
    "      2 \n",
    "----> 3 dfii.select(dfii.n + dfii.abool).show(5)\n",
    "      4 \n",
    "      5 # OUTPUT: Error\n",
    "\n",
    "/usr/local/anaconda3/lib/python3.7/site-packages/pyspark/sql/dataframe.py in select(self, *cols)\n",
    "   1319         [Row(name=u'Alice', age=12), Row(name=u'Bob', age=15)]\n",
    "   1320         \"\"\"\n",
    "-> 1321         jdf = self._jdf.select(self._jcols(*cols))\n",
    "   1322         return DataFrame(jdf, self.sql_ctx)\n",
    "   1323 \n",
    "\n",
    "/usr/local/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py in __call__(self, *args)\n",
    "   1255         answer = self.gateway_client.send_command(command)\n",
    "   1256         return_value = get_return_value(\n",
    "-> 1257             answer, self.gateway_client, self.target_id, self.name)\n",
    "   1258 \n",
    "   1259         for temp_arg in temp_args:\n",
    "\n",
    "/usr/local/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py in deco(*a, **kw)\n",
    "     67                                              e.java_exception.getStackTrace()))\n",
    "     68             if s.startswith('org.apache.spark.sql.AnalysisException: '):\n",
    "---> 69                 raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
    "     70             if s.startswith('org.apache.spark.sql.catalyst.analysis'):\n",
    "     71                 raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
    "\n",
    "AnalysisException: \"cannot resolve '(`n` + `abool`)' due to data type mismatch: differing types in '(`n` + `abool`)' (double and boolean).;;\\n'Project [(n#178 + abool#180) AS (n + abool)#285]\\n+- Project [n#178, group#179, abool#180, n2#238, POWER(n#178, cast(2 as double)) AS n3#256]\\n   +- Project [n#178, group#179, abool#180, (n#178 * cast(-1 as double)) AS n2#238]\\n      +- LogicalRDD [n#178, group#179, abool#180], false\\n\"</code>\n",
    "\n",
    "[Return](#Output-II.I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example III.G\n",
    "[Return](#Output-III.G)\n",
    "\n",
    "<Code>---------------------------------------------------------------------------\n",
    "Py4JJavaError                             Traceback (most recent call last)\n",
    "/usr/local/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py in deco(*a, **kw)\n",
    "     62         try:\n",
    "---> 63             return f(*a, **kw)\n",
    "     64         except py4j.protocol.Py4JJavaError as e:\n",
    "\n",
    "/usr/local/anaconda3/lib/python3.7/site-packages/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)\n",
    "    327                     \"An error occurred while calling {0}{1}{2}.\\n\".\n",
    "--> 328                     format(target_id, \".\", name), value)\n",
    "    329             else:\n",
    "\n",
    "Py4JJavaError: An error occurred while calling o20.sql.\n",
    ": org.apache.spark.sql.AnalysisException: cannot resolve '`l`' given input columns: [my_df.n, my_df.group, my_df.abool]; line 2 pos 7;\n",
    "'Project ['l, n#284, (n#284 / cast(2 as double)) AS n2#397, (n#284 - cast(1 as double)) AS n3#398]\n",
    "+- SubqueryAlias `my_df`\n",
    "   +- LogicalRDD [n#284, group#285, abool#286], false\n",
    "\n",
    "\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)\n",
    "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:280)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n",
    "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n",
    "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
    "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
    "\tat scala.collection.immutable.List.foreach(List.scala:392)\n",
    "\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
    "\tat scala.collection.immutable.List.map(List.scala:296)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n",
    "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n",
    "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n",
    "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n",
    "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n",
    "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n",
    "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\n",
    "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
    "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
    "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
    "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
    "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
    "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
    "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
    "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
    "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
    "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
    "\tat java.lang.Thread.run(Thread.java:748)\n",
    "\n",
    "\n",
    "During handling of the above exception, another exception occurred:\n",
    "\n",
    "AnalysisException                         Traceback (most recent call last)\n",
    "<ipython-input-40-0a9fc38cab0c> in <module>\n",
    "      3 spark.sql('''\n",
    "      4 SELECT l, n, (n/2) as n2, (n-1) as n3 FROM my_df\n",
    "----> 5 ''').show(5)\n",
    "\n",
    "/usr/local/anaconda3/lib/python3.7/site-packages/pyspark/sql/session.py in sql(self, sqlQuery)\n",
    "    765         [Row(f1=1, f2=u'row1'), Row(f1=2, f2=u'row2'), Row(f1=3, f2=u'row3')]\n",
    "    766         \"\"\"\n",
    "--> 767         return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n",
    "    768 \n",
    "    769     @since(2.0)\n",
    "\n",
    "/usr/local/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py in __call__(self, *args)\n",
    "   1255         answer = self.gateway_client.send_command(command)\n",
    "   1256         return_value = get_return_value(\n",
    "-> 1257             answer, self.gateway_client, self.target_id, self.name)\n",
    "   1258 \n",
    "   1259         for temp_arg in temp_args:\n",
    "\n",
    "/usr/local/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py in deco(*a, **kw)\n",
    "     67                                              e.java_exception.getStackTrace()))\n",
    "     68             if s.startswith('org.apache.spark.sql.AnalysisException: '):\n",
    "---> 69                 raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
    "     70             if s.startswith('org.apache.spark.sql.catalyst.analysis'):\n",
    "     71                 raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
    "\n",
    "AnalysisException: \"cannot resolve '`l`' given input columns: [my_df.n, my_df.group, my_df.abool]; line 2 pos 7;\\n'Project ['l, n#284, (n#284 / cast(2 as double)) AS n2#397, (n#284 - cast(1 as double)) AS n3#398]\\n+- SubqueryAlias `my_df`\\n   +- LogicalRDD [n#284, group#285, abool#286], false\\n\"</code>\n",
    "\n",
    "[Return](#Output-III.G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
