{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark API Mini Exercises\n",
    "\n",
    "### Contents\n",
    "- [Section I - Spark Dataframe Basics](#Section-I)\n",
    "- [Section II - Column Manipulation](#Section-II)\n",
    "- [Section III - Spark SQL](#Section-III)\n",
    "- [Section IV - Type casting](#Section-IV)\n",
    "- [Section V - Built-in Functions](#Section-V)\n",
    "- [Section VI - Filter / Where](#Section-VI)\n",
    "- [Section VII - When / Otherwise](#Section-VII)\n",
    "- [Section VIII - Sorting](#Section-VIII)\n",
    "- [Section IX - Aggregating](#Section-IX)\n",
    "- [Appendix](#Appendix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the code below to create a pandas dataframe with 20 rows and 3 columns:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(13)\n",
    "\n",
    "pandas_dataframe = pd.DataFrame(\n",
    "    {\n",
    "        \"n\": np.random.randn(20),\n",
    "        \"group\": np.random.choice(list(\"xyz\"), 20),\n",
    "        \"abool\": np.random.choice([True, False], 20),\n",
    "    }\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h1 border=1>Section I</h1>\n",
    "\n",
    "## <mark>Spark Dataframe Basics</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.A \n",
    "Use the starter code above to create a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(13)\n",
    "\n",
    "pandas_dataframe = pd.DataFrame(\n",
    "    {\n",
    "        \"n\": np.random.randn(20),\n",
    "        \"group\": np.random.choice(list(\"xyz\"), 20),\n",
    "        \"abool\": np.random.choice([True, False], 20),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.B\n",
    "Convert the pandas dataframe to a spark dataframe. From this point forward, do all of your work with the spark dataframe, not the pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi = spark.createDataFrame(pandas_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.C \n",
    "Show the first 3 rows of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "|  -0.712390662050588|    z|false|\n",
      "|   0.753766378659703|    x|false|\n",
      "|-0.04450307833805...|    z|false|\n",
      "+--------------------+-----+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfi.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.D \n",
    "Show the first 7 rows of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "|  -0.712390662050588|    z|false|\n",
      "|   0.753766378659703|    x|false|\n",
      "|-0.04450307833805...|    z|false|\n",
      "| 0.45181233874578974|    y|false|\n",
      "|  1.3451017084510097|    z|false|\n",
      "|  0.5323378882945463|    y|false|\n",
      "|  1.3501878997225267|    z|false|\n",
      "+--------------------+-----+-----+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfi.show(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.E \n",
    "View a summary of the data using `.describe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-----+\n",
      "|summary|                  n|group|\n",
      "+-------+-------------------+-----+\n",
      "|  count|                 20|   20|\n",
      "|   mean|0.36640264498852165| null|\n",
      "| stddev| 0.8905322898155364| null|\n",
      "|    min| -1.261605945319069|    x|\n",
      "|    max| 2.1503829673811126|    z|\n",
      "+-------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfi.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.F \n",
    "Use .select to create a new dataframe with just the `n` and `abool` columns. View the first 5 rows of this dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                   n|abool|\n",
      "+--------------------+-----+\n",
      "|  -0.712390662050588|false|\n",
      "|   0.753766378659703|false|\n",
      "|-0.04450307833805...|false|\n",
      "| 0.45181233874578974|false|\n",
      "|  1.3451017084510097|false|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfi_nabool = dfi.select('n','abool')\n",
    "dfi_nabool.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.G \n",
    "Use `.select` to create a new dataframe with just the `group` and `abool` columns. View the first 5 rows of this dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|group|abool|\n",
      "+-----+-----+\n",
      "|    z|false|\n",
      "|    x|false|\n",
      "|    z|false|\n",
      "|    y|false|\n",
      "|    z|false|\n",
      "+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfi_groupabool = dfi.select('group','abool')\n",
    "dfi_groupabool.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.H\n",
    "Use `.select` to create a new dataframe with the `group` column and the `abool` column renamed to `a_boolean_value`. Show the first 3 rows of this dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+\n",
      "|group|a_boolean_value|\n",
      "+-----+---------------+\n",
      "|    z|          false|\n",
      "|    x|          false|\n",
      "|    z|          false|\n",
      "+-----+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfi_groupaboolean = dfi.select('group',dfi.abool.alias('a_boolean_value'))\n",
    "dfi_groupaboolean.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.J \n",
    "Use `.select` to create a new dataframe with the `group` column and the `n` column renamed to `a_numeric_value`. Show the first 6 rows of this dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|group|     a_numeric_value|\n",
      "+-----+--------------------+\n",
      "|    z|  -0.712390662050588|\n",
      "|    x|   0.753766378659703|\n",
      "|    z|-0.04450307833805...|\n",
      "|    y| 0.45181233874578974|\n",
      "|    z|  1.3451017084510097|\n",
      "|    y|  0.5323378882945463|\n",
      "+-----+--------------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfi_groupnum = dfi.select('group',dfi.n.alias('a_numeric_value'))\n",
    "dfi_groupnum.show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section II\n",
    "## Column Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.A. \n",
    "Use the starter code above to re-create a spark dataframe. Store the spark dataframe in a varaible named df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfii = spark.createDataFrame(pandas_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.B\n",
    "Use .select to add 4 to the `n` column. Show the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+------------------+\n",
      "|                   n|group|abool|          n_plus_4|\n",
      "+--------------------+-----+-----+------------------+\n",
      "|  -0.712390662050588|    z|false|3.2876093379494122|\n",
      "|   0.753766378659703|    x|false| 4.753766378659703|\n",
      "|-0.04450307833805...|    z|false|3.9554969216619464|\n",
      "| 0.45181233874578974|    y|false|  4.45181233874579|\n",
      "|  1.3451017084510097|    z|false|5.3451017084510095|\n",
      "+--------------------+-----+-----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_plus_4 = (dfii.n+4).alias('n_plus_4')\n",
    "dfii.select('*', n_plus_4).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.C\n",
    "Subtract 5 from the `n` column and view the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+-------------------+\n",
      "|                   n|group|abool|           n_less_5|\n",
      "+--------------------+-----+-----+-------------------+\n",
      "|  -0.712390662050588|    z|false| -5.712390662050588|\n",
      "|   0.753766378659703|    x|false| -4.246233621340297|\n",
      "|-0.04450307833805...|    z|false| -5.044503078338053|\n",
      "| 0.45181233874578974|    y|false|  -4.54818766125421|\n",
      "|  1.3451017084510097|    z|false|-3.6548982915489905|\n",
      "+--------------------+-----+-----+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_less_5 = (dfii.n - 5).alias('n_less_5')\n",
    "dfii.select('*', n_less_5).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.D \n",
    "Multiply the `n` column by 2. View the results along with the original numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+--------------------+\n",
      "|                   n|group|abool|           n_times_2|\n",
      "+--------------------+-----+-----+--------------------+\n",
      "|  -0.712390662050588|    z|false|  -1.424781324101176|\n",
      "|   0.753766378659703|    x|false|   1.507532757319406|\n",
      "|-0.04450307833805...|    z|false|-0.08900615667610691|\n",
      "| 0.45181233874578974|    y|false|  0.9036246774915795|\n",
      "|  1.3451017084510097|    z|false|  2.6902034169020195|\n",
      "+--------------------+-----+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_times_2 = (dfii.n * 2).alias('n_times_2')\n",
    "dfii.select('*', n_times_2).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.E \n",
    "Add a new column named `n2` that is the `n` value multiplied by -1. Show the first 4 rows of your dataframe. You should see the original `n` value as well as `n2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+--------------------+\n",
      "|                   n|group|abool|                  n2|\n",
      "+--------------------+-----+-----+--------------------+\n",
      "|  -0.712390662050588|    z|false|   0.712390662050588|\n",
      "|   0.753766378659703|    x|false|  -0.753766378659703|\n",
      "|-0.04450307833805...|    z|false|0.044503078338053455|\n",
      "| 0.45181233874578974|    y|false|-0.45181233874578974|\n",
      "+--------------------+-----+-----+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n2 = (dfii.n * -1).alias('n2')\n",
    "dfii = dfii.select('*', n2)\n",
    "dfii.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.F\n",
    "Add a new column named `n3` that is the `n` value squared. Show the first 5 rows of your dataframe. You should see both `n`, `n2`, and `n3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+--------------------+--------------------+\n",
      "|                   n|group|abool|                  n2|                  n3|\n",
      "+--------------------+-----+-----+--------------------+--------------------+\n",
      "|  -0.712390662050588|    z|false|   0.712390662050588|   0.507500455376875|\n",
      "|   0.753766378659703|    x|false|  -0.753766378659703|  0.5681637535977627|\n",
      "|-0.04450307833805...|    z|false|0.044503078338053455|0.001980523981562...|\n",
      "| 0.45181233874578974|    y|false|-0.45181233874578974| 0.20413438944294027|\n",
      "|  1.3451017084510097|    z|false| -1.3451017084510097|  1.8092986060778251|\n",
      "+--------------------+-----+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n3 = (dfii.n ** 2).alias('n3')\n",
    "dfii = dfii.select('*', n3)\n",
    "dfii.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.G \n",
    "What happens when you run the code below?\n",
    "```python\n",
    "df.group + df.abool\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'(group + abool)'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfii.group + dfii.abool\n",
    "\n",
    "# OUTPUT: Column<b'(group + abool)'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.H\n",
    "What happens when you run the code below? What is the difference between this and the previous code sample?\n",
    "```python\n",
    "df.select(df.group + df.abool)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfii.select(dfii.group + dfii.abool)\n",
    "\n",
    "# OUTPUT: ERROR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output II.H\n",
    "[See error](#Example-II.H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.I\n",
    "Try adding various other columns together. What are the results of combining the different data types?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfii.select(dfii.n + dfii.abool).show(5)\n",
    "\n",
    "# OUTPUT: Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output II.I\n",
    "[See error](#Example-II.I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            (n + n3)|\n",
      "+--------------------+\n",
      "|-0.20489020667371294|\n",
      "|  1.3219301322574657|\n",
      "|-0.04252255435649...|\n",
      "|    0.65594672818873|\n",
      "|   3.154400314528835|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfii.select(dfii.n + dfii.n3).show(5)\n",
    "\n",
    "# OUTPUT: Mathemagics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section III\n",
    "## Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. \n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe.\n",
    "    1. Turn your dataframe into a table that can be queried with spark SQL. Name\n",
    "       the table `my_df`. Answer the rest of the questions in this section with\n",
    "       a spark sql query (`spark.sql`) against `my_df`. After each step, view\n",
    "       the first 7 records from the dataframe.\n",
    "    1. Write a query that shows all of the columns from your dataframe.\n",
    "    1. Write a query that shows just the `n` and `abool` columns from the\n",
    "       dataframe.\n",
    "    1. Write a query that shows just the `n` and `group` columns. Rename the\n",
    "       `group` column to `g`.\n",
    "    1. Write a query that selects `n`, and creates two new columns: `n2`, the\n",
    "       original `n` values halved, and `n3`: the original n values minus 1.\n",
    "    1. What happens if you make a SQL syntax error in your query?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col, expr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.A \n",
    "Use the starter code above to re-create a spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfiii = spark.createDataFrame(pandas_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.B\n",
    "Turn your dataframe into a table that can be queried with spark SQL. Name the table `my_df`. Answer the rest of the questions in this section with a spark sql query (`spark.sql`) against `my_df`. After each step, view the first 7 records from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "|  -0.712390662050588|    z|false|\n",
      "|   0.753766378659703|    x|false|\n",
      "|-0.04450307833805...|    z|false|\n",
      "| 0.45181233874578974|    y|false|\n",
      "|  1.3451017084510097|    z|false|\n",
      "|  0.5323378882945463|    y|false|\n",
      "|  1.3501878997225267|    z|false|\n",
      "+--------------------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_df = dfiii\n",
    "# display(my_df.show())\n",
    "my_df.createOrReplaceTempView(\"my_df\")\n",
    "\n",
    "spark.sql('''\n",
    "SELECT * FROM my_df LIMIT 7\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.C\n",
    "Write a query that shows all of the columns from your dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "|  -0.712390662050588|    z|false|\n",
      "|   0.753766378659703|    x|false|\n",
      "|-0.04450307833805...|    z|false|\n",
      "| 0.45181233874578974|    y|false|\n",
      "|  1.3451017084510097|    z|false|\n",
      "+--------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT * FROM my_df\n",
    "''').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.D \n",
    "Write a query that shows just the `n` and `abool` columns from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                   n|abool|\n",
      "+--------------------+-----+\n",
      "|  -0.712390662050588|false|\n",
      "|   0.753766378659703|false|\n",
      "|-0.04450307833805...|false|\n",
      "| 0.45181233874578974|false|\n",
      "|  1.3451017084510097|false|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT n, abool FROM my_df\n",
    "''').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.E \n",
    "Write a query that shows just the `n` and `group` columns. Rename the group column to `g`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|                   n|  g|\n",
      "+--------------------+---+\n",
      "|  -0.712390662050588|  z|\n",
      "|   0.753766378659703|  x|\n",
      "|-0.04450307833805...|  z|\n",
      "| 0.45181233874578974|  y|\n",
      "|  1.3451017084510097|  z|\n",
      "+--------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT n, group as g FROM my_df\n",
    "''').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.F \n",
    "Write a query that selects `n`, and creates two new columns: `n2`, the original n values halved, and `n3`: the original n values minus 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                   n|                  n2|                  n3|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|  -0.712390662050588|  -0.356195331025294|  -1.712390662050588|\n",
      "|   0.753766378659703|  0.3768831893298515|-0.24623362134029703|\n",
      "|-0.04450307833805...|-0.02225153916902...| -1.0445030783380536|\n",
      "| 0.45181233874578974| 0.22590616937289487| -0.5481876612542103|\n",
      "|  1.3451017084510097|  0.6725508542255049| 0.34510170845100974|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT n, (n/2) as n2, (n-1) as n3 FROM my_df\n",
    "''').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.G \n",
    "What happens if you make a SQL syntax error in your query?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql('''\n",
    "# SELECT l, n, (n/2) as n2, (n-1) as n3 FROM my_df\n",
    "# ''').show(5)\n",
    "\n",
    "# OUTPUT: ERROR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output III.G\n",
    "[see error](#Example-III.G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h1>Section IV</h1>\n",
    "<h2><mark>Type casting</mark></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.A\n",
    "Use the starter code above to re-create a spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfiv = spark.createDataFrame(pandas_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.B\n",
    "Use `.printSchema` to view the datatypes in your dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- n: double (nullable = true)\n",
      " |-- group: string (nullable = true)\n",
      " |-- abool: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfiv.printSchema()\n",
    "\n",
    "# root\n",
    "#  |-- n: double (nullable = true)\n",
    "#  |-- group: string (nullable = true)\n",
    "#  |-- abool: boolean (nullable = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.C\n",
    "Use `.dtypes` to view the datatypes in your dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('n', 'double'), ('group', 'string'), ('abool', 'boolean')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfiv.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.D\n",
    "What is the difference between the two code samples below?\n",
    "\n",
    "    ```python\n",
    "    df.abool.cast('int')\n",
    "    ```\n",
    "\n",
    "    ```python\n",
    "    df.select(df.abool.cast('int')).show()\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'CAST(abool AS INT)'>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfiv.abool.cast('int')\n",
    "\n",
    "# references a spark column object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|abool|\n",
      "+-----+\n",
      "|    0|\n",
      "|    0|\n",
      "|    0|\n",
      "|    0|\n",
      "|    0|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfiv.select(dfiv.abool.cast('int')).show(5)\n",
    "\n",
    "# Displays a spark dataframe object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.E\n",
    "Use `.select` and `.cast` to convert the `abool` column to an integer type. View the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "|  -0.712390662050588|    z|    0|\n",
      "|   0.753766378659703|    x|    0|\n",
      "|-0.04450307833805...|    z|    0|\n",
      "| 0.45181233874578974|    y|    0|\n",
      "|  1.3451017084510097|    z|    0|\n",
      "+--------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfiv.select(dfiv.n, dfiv.group, dfiv.abool.cast('int')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.F\n",
    "Convert the `group` column to a integer data type and view the results. What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|group|\n",
      "+-----+\n",
      "| null|\n",
      "| null|\n",
      "| null|\n",
      "| null|\n",
      "| null|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfiv.select(dfiv.group.cast('int')).show(5)\n",
    "\n",
    "# Displays a spark dataframe object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.H\n",
    "Convert the `n` column to a integer data type and view the results. What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|  n|\n",
      "+---+\n",
      "|  0|\n",
      "|  0|\n",
      "|  0|\n",
      "|  0|\n",
      "|  1|\n",
      "+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfiv.select(dfiv.n.cast('int')).show(5)\n",
    "\n",
    "# Everything rounds towards zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.I\n",
    "Convert the `abool` column to a string data type and view the results. What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|abool|\n",
      "+-----+\n",
      "|false|\n",
      "|false|\n",
      "|false|\n",
      "|false|\n",
      "|false|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfiv.select(dfiv.abool.cast('string')).show(5)\n",
    "\n",
    "# Looks the same ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- abool: boolean (nullable = true)\n",
      " |-- peekabool: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfiv.select(dfiv.abool, dfiv.abool.cast('string').alias('peekabool')).printSchema()\n",
    "\n",
    "# ... but isn't"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section V\n",
    "## Built-in Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V.A\n",
    "Use the starter code above to re-create a spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfv = spark.createDataFrame(pandas_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V.B\n",
    "Import the necessary functions from `pyspark.sql.functions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V.C\n",
    "Find the highest `n` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|            max(n)|\n",
      "+------------------+\n",
      "|2.1503829673811126|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfv.select(F.max(dfv.n)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V.D\n",
    "Find the lowest `n` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|            min(n)|\n",
      "+------------------+\n",
      "|-1.261605945319069|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfv.select(F.min(dfv.n)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V.E\n",
    "Find the average `n` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|             avg(n)|\n",
      "+-------------------+\n",
      "|0.36640264498852165|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfv.select(F.avg(dfv.n)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V.F\n",
    "Use `concat` to change the `group` column to say, e.g. \"Group: x\" or \"Group: y\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|  group|\n",
      "+-------+\n",
      "|Group z|\n",
      "|Group x|\n",
      "|Group z|\n",
      "|Group y|\n",
      "|Group z|\n",
      "+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfv.select(F.concat(F.lit('Group '), dfv.group).alias('group')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V.G\n",
    "Use `concat` to combine the `n` and `group` columns to produce results that look like this: \"x: -1.432\" or \"z: 2.352\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|   groupn|\n",
      "+---------+\n",
      "|z: -0.712|\n",
      "| x: 0.754|\n",
      "|z: -0.045|\n",
      "| y: 0.452|\n",
      "| z: 1.345|\n",
      "+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfv.select(F.concat(dfv.group, F.lit(': '), F.round(dfv.n,3)).alias('groupn')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section VI\n",
    "## Filter / Where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI.A\n",
    "Use the starter code above to re-create a spark dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfvi = spark.createDataFrame(pandas_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI.B\n",
    "Use `.filter` or `.where` to select just the rows where the group is `y` and view the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "| 0.45181233874578974|    y|false|\n",
      "|  0.5323378882945463|    y|false|\n",
      "| -1.0453771305385342|    y| true|\n",
      "|  -1.261605945319069|    y|false|\n",
      "|  0.5628467852810314|    y| true|\n",
      "|-0.24332625188556253|    y| true|\n",
      "|  0.9137407048596775|    y|false|\n",
      "|  2.1503829673811126|    y| true|\n",
      "+--------------------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfvi.filter(dfvi.group=='y').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI.C\n",
    "Select just the columns where the `abool` column is false and view the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "|  -0.712390662050588|    z|false|\n",
      "|   0.753766378659703|    x|false|\n",
      "|-0.04450307833805...|    z|false|\n",
      "| 0.45181233874578974|    y|false|\n",
      "|  1.3451017084510097|    z|false|\n",
      "|  0.5323378882945463|    y|false|\n",
      "|  1.3501878997225267|    z|false|\n",
      "|  0.8612113741693206|    x|false|\n",
      "| -0.7889890249515489|    x|false|\n",
      "|  -1.261605945319069|    y|false|\n",
      "|  0.9137407048596775|    y|false|\n",
      "| 0.31735092273633597|    x|false|\n",
      "| 0.12730328020698067|    z|false|\n",
      "|  0.6062886568962988|    x|false|\n",
      "+--------------------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfvi.filter(dfvi.abool==False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI.D\n",
    "Find the columns where the `group` column is *not* `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "|  -0.712390662050588|    z|false|\n",
      "|   0.753766378659703|    x|false|\n",
      "|-0.04450307833805...|    z|false|\n",
      "|  1.3451017084510097|    z|false|\n",
      "|  1.3501878997225267|    z|false|\n",
      "|  0.8612113741693206|    x|false|\n",
      "|  1.4786857374358966|    z| true|\n",
      "| -0.7889890249515489|    x|false|\n",
      "| 0.31735092273633597|    x|false|\n",
      "| 0.12730328020698067|    z|false|\n",
      "|  0.6062886568962988|    x|false|\n",
      "|-0.02677164998644...|    x| true|\n",
      "+--------------------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfvi.filter(dfvi.group!='y').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI.E\n",
    "Find the columns where `n` is positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-----+\n",
      "|                  n|group|abool|\n",
      "+-------------------+-----+-----+\n",
      "|  0.753766378659703|    x|false|\n",
      "|0.45181233874578974|    y|false|\n",
      "| 1.3451017084510097|    z|false|\n",
      "| 0.5323378882945463|    y|false|\n",
      "| 1.3501878997225267|    z|false|\n",
      "| 0.8612113741693206|    x|false|\n",
      "| 1.4786857374358966|    z| true|\n",
      "| 0.5628467852810314|    y| true|\n",
      "| 0.9137407048596775|    y|false|\n",
      "|0.31735092273633597|    x|false|\n",
      "|0.12730328020698067|    z|false|\n",
      "| 2.1503829673811126|    y| true|\n",
      "| 0.6062886568962988|    x|false|\n",
      "+-------------------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfvi.filter(dfvi.n>0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI.F\n",
    "Find the columns where `abool` is true and the `group` column is `z`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+-----+\n",
      "|                 n|group|abool|\n",
      "+------------------+-----+-----+\n",
      "|1.4786857374358966|    z| true|\n",
      "+------------------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfvi.filter(dfvi.group=='z').where(dfvi.abool==True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI.G\n",
    "Find the columns where `abool` is true or the `group` column is `z`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "|  -0.712390662050588|    z|false|\n",
      "|-0.04450307833805...|    z|false|\n",
      "|  1.3451017084510097|    z|false|\n",
      "|  1.3501878997225267|    z|false|\n",
      "|  1.4786857374358966|    z| true|\n",
      "| -1.0453771305385342|    y| true|\n",
      "|  0.5628467852810314|    y| true|\n",
      "|-0.24332625188556253|    y| true|\n",
      "| 0.12730328020698067|    z|false|\n",
      "|  2.1503829673811126|    y| true|\n",
      "|-0.02677164998644...|    x| true|\n",
      "+--------------------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfvi.where((dfvi.abool==True) | (dfvi.group=='z')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI.H\n",
    "Find the columns where `abool` is false and `n` is less than 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "|  -0.712390662050588|    z|false|\n",
      "|   0.753766378659703|    x|false|\n",
      "|-0.04450307833805...|    z|false|\n",
      "| 0.45181233874578974|    y|false|\n",
      "|  0.5323378882945463|    y|false|\n",
      "|  0.8612113741693206|    x|false|\n",
      "| -0.7889890249515489|    x|false|\n",
      "|  -1.261605945319069|    y|false|\n",
      "|  0.9137407048596775|    y|false|\n",
      "| 0.31735092273633597|    x|false|\n",
      "| 0.12730328020698067|    z|false|\n",
      "|  0.6062886568962988|    x|false|\n",
      "+--------------------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfvi.filter(dfvi.n<1).where(dfvi.abool==False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI.I\n",
    "Find the columns where `abool` is false or `n` is less than 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "|  -0.712390662050588|    z|false|\n",
      "|   0.753766378659703|    x|false|\n",
      "|-0.04450307833805...|    z|false|\n",
      "| 0.45181233874578974|    y|false|\n",
      "|  1.3451017084510097|    z|false|\n",
      "|  0.5323378882945463|    y|false|\n",
      "|  1.3501878997225267|    z|false|\n",
      "|  0.8612113741693206|    x|false|\n",
      "| -1.0453771305385342|    y| true|\n",
      "| -0.7889890249515489|    x|false|\n",
      "|  -1.261605945319069|    y|false|\n",
      "|  0.5628467852810314|    y| true|\n",
      "|-0.24332625188556253|    y| true|\n",
      "|  0.9137407048596775|    y|false|\n",
      "| 0.31735092273633597|    x|false|\n",
      "| 0.12730328020698067|    z|false|\n",
      "|  0.6062886568962988|    x|false|\n",
      "|-0.02677164998644...|    x| true|\n",
      "+--------------------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfvi.where((dfvi.abool==False) | (dfvi.n<1)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section VII\n",
    "## When / Otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VII.A\n",
    "Use the starter code above to re-create a spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfvii = spark.createDataFrame(pandas_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VII.B\n",
    "Use `when` and `.otherwise` to create a column that contains the text \"It is true\" when `abool` is true and \"It is false\"\" when `abool` is false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+-----------+\n",
      "|                   n|group|abool|    itiswha|\n",
      "+--------------------+-----+-----+-----------+\n",
      "|  -0.712390662050588|    z|false|It is false|\n",
      "|   0.753766378659703|    x|false|It is false|\n",
      "|-0.04450307833805...|    z|false|It is false|\n",
      "| 0.45181233874578974|    y|false|It is false|\n",
      "|  1.3451017084510097|    z|false|It is false|\n",
      "|  0.5323378882945463|    y|false|It is false|\n",
      "|  1.3501878997225267|    z|false|It is false|\n",
      "|  0.8612113741693206|    x|false|It is false|\n",
      "|  1.4786857374358966|    z| true| It is true|\n",
      "| -1.0453771305385342|    y| true| It is true|\n",
      "| -0.7889890249515489|    x|false|It is false|\n",
      "|  -1.261605945319069|    y|false|It is false|\n",
      "|  0.5628467852810314|    y| true| It is true|\n",
      "|-0.24332625188556253|    y| true| It is true|\n",
      "|  0.9137407048596775|    y|false|It is false|\n",
      "+--------------------+-----+-----+-----------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfvii.select('*', F.when(dfvii.abool, 'It is true').otherwise('It is false').alias('itiswha')).show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VII.C\n",
    "Create a column that contains 0 if n is less than 0, otherwise, the original n value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+-------------------+\n",
      "|                   n|group|abool|          positiven|\n",
      "+--------------------+-----+-----+-------------------+\n",
      "|  -0.712390662050588|    z|false|                0.0|\n",
      "|   0.753766378659703|    x|false|  0.753766378659703|\n",
      "|-0.04450307833805...|    z|false|                0.0|\n",
      "| 0.45181233874578974|    y|false|0.45181233874578974|\n",
      "|  1.3451017084510097|    z|false| 1.3451017084510097|\n",
      "|  0.5323378882945463|    y|false| 0.5323378882945463|\n",
      "|  1.3501878997225267|    z|false| 1.3501878997225267|\n",
      "|  0.8612113741693206|    x|false| 0.8612113741693206|\n",
      "|  1.4786857374358966|    z| true| 1.4786857374358966|\n",
      "| -1.0453771305385342|    y| true|                0.0|\n",
      "| -0.7889890249515489|    x|false|                0.0|\n",
      "|  -1.261605945319069|    y|false|                0.0|\n",
      "|  0.5628467852810314|    y| true| 0.5628467852810314|\n",
      "|-0.24332625188556253|    y| true|                0.0|\n",
      "|  0.9137407048596775|    y|false| 0.9137407048596775|\n",
      "+--------------------+-----+-----+-------------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfvii.select('*', F.when(dfvii.n<0, 0).otherwise(dfvii.n).alias('positiven')).show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section VIII\n",
    "## Sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VIII.A\n",
    "Use the starter code above to re-create a spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfviii = spark.createDataFrame(pandas_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VIII.B\n",
    "Sort by the `n` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "|  -1.261605945319069|    y|false|\n",
      "| -1.0453771305385342|    y| true|\n",
      "| -0.7889890249515489|    x|false|\n",
      "|  -0.712390662050588|    z|false|\n",
      "|-0.24332625188556253|    y| true|\n",
      "|-0.04450307833805...|    z|false|\n",
      "|-0.02677164998644...|    x| true|\n",
      "| 0.12730328020698067|    z|false|\n",
      "| 0.31735092273633597|    x|false|\n",
      "| 0.45181233874578974|    y|false|\n",
      "+--------------------+-----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfviii.sort(dfviii.n).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VIII.C\n",
    "Sort by the `group` value, both ascending and descending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "|   0.753766378659703|    x|false|\n",
      "| -0.7889890249515489|    x|false|\n",
      "|-0.02677164998644...|    x| true|\n",
      "|  0.6062886568962988|    x|false|\n",
      "| 0.31735092273633597|    x|false|\n",
      "|  0.8612113741693206|    x|false|\n",
      "|  -1.261605945319069|    y|false|\n",
      "|  0.9137407048596775|    y|false|\n",
      "|  0.5628467852810314|    y| true|\n",
      "| 0.45181233874578974|    y|false|\n",
      "+--------------------+-----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfviii.sort(dfviii.group).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+-----+\n",
      "|                 n|group|abool|\n",
      "+------------------+-----+-----+\n",
      "|2.1503829673811126|    y| true|\n",
      "|1.4786857374358966|    z| true|\n",
      "|1.3501878997225267|    z|false|\n",
      "|1.3451017084510097|    z|false|\n",
      "|0.9137407048596775|    y|false|\n",
      "|0.8612113741693206|    x|false|\n",
      "| 0.753766378659703|    x|false|\n",
      "|0.6062886568962988|    x|false|\n",
      "|0.5628467852810314|    y| true|\n",
      "|0.5323378882945463|    y|false|\n",
      "+------------------+-----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfviii.sort(dfviii.n.desc()).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VIII.D\n",
    "Sort by the group value first, then, within each group, sort by `n` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "| -0.7889890249515489|    x|false|\n",
      "|-0.02677164998644...|    x| true|\n",
      "| 0.31735092273633597|    x|false|\n",
      "|  0.6062886568962988|    x|false|\n",
      "|   0.753766378659703|    x|false|\n",
      "|  0.8612113741693206|    x|false|\n",
      "|  -1.261605945319069|    y|false|\n",
      "| -1.0453771305385342|    y| true|\n",
      "|-0.24332625188556253|    y| true|\n",
      "| 0.45181233874578974|    y|false|\n",
      "+--------------------+-----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfviii.sort(dfviii.group, dfviii.n).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VIII.E\n",
    "Sort by `abool`, `group`, and `n`. Does it matter in what order you specify the columns when sorting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-----+\n",
      "|                  n|group|abool|\n",
      "+-------------------+-----+-----+\n",
      "|-0.7889890249515489|    x|false|\n",
      "|0.31735092273633597|    x|false|\n",
      "| 0.6062886568962988|    x|false|\n",
      "|  0.753766378659703|    x|false|\n",
      "| 0.8612113741693206|    x|false|\n",
      "| -1.261605945319069|    y|false|\n",
      "|0.45181233874578974|    y|false|\n",
      "| 0.5323378882945463|    y|false|\n",
      "| 0.9137407048596775|    y|false|\n",
      "| -0.712390662050588|    z|false|\n",
      "+-------------------+-----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfviii.sort(dfviii.abool, dfviii.group, dfviii.n).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "| -0.7889890249515489|    x|false|\n",
      "| 0.31735092273633597|    x|false|\n",
      "|  0.6062886568962988|    x|false|\n",
      "|   0.753766378659703|    x|false|\n",
      "|  0.8612113741693206|    x|false|\n",
      "|-0.02677164998644...|    x| true|\n",
      "|  -1.261605945319069|    y|false|\n",
      "| 0.45181233874578974|    y|false|\n",
      "|  0.5323378882945463|    y|false|\n",
      "|  0.9137407048596775|    y|false|\n",
      "+--------------------+-----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfviii.sort(dfviii.group, dfviii.abool, dfviii.n).show(10)\n",
    "\n",
    "# YES! Order matters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|group|\n",
      "+-----+\n",
      "|    x|\n",
      "|    z|\n",
      "|    y|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfviii.select('group').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section IX\n",
    "## Aggregating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfix = spark.createDataFrame(pandas_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IX.A \n",
    "What is the average `n` value for each group in the `group` column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+\n",
      "|group|             avg(n)|\n",
      "+-----+-------------------+\n",
      "|    x|0.28714277625394485|\n",
      "|    z|  0.590730814237962|\n",
      "|    y| 0.2576014196023739|\n",
      "+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfix.groupby('group').agg(F.avg('n')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IX.B\n",
    "What is the maximum `n` value for each group in the `group` column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|group|            max(n)|\n",
      "+-----+------------------+\n",
      "|    x|0.8612113741693206|\n",
      "|    z|1.4786857374358966|\n",
      "|    y|2.1503829673811126|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfix.groupby('group').agg(F.max('n')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IX.C\n",
    "What is the minimum `n` value by `abool`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+\n",
      "|group|             min(n)|\n",
      "+-----+-------------------+\n",
      "|    x|-0.7889890249515489|\n",
      "|    z| -0.712390662050588|\n",
      "|    y| -1.261605945319069|\n",
      "+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfix.groupby('group').agg(F.min('n')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IX.D\n",
    "What is the average `n` value for each unique combination of the `group` and `abool` column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+--------------------+\n",
      "|group|abool|              avg(n)|\n",
      "+-----+-----+--------------------+\n",
      "|    x| true|-0.02677164998644...|\n",
      "|    x|false|   0.349925661502022|\n",
      "|    y| true| 0.35613159255951177|\n",
      "|    y|false| 0.15907124664523611|\n",
      "|    z| true|  1.4786857374358966|\n",
      "|    z|false| 0.41313982959837514|\n",
      "+-----+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfix.groupby('group', 'abool').agg(F.avg('n')).sort('group',dfix.abool.desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example II.H\n",
    "[Return](#II.H)\n",
    "\n",
    "\n",
    "<code>---------------------------------------------------------------------------\n",
    "Py4JJavaError                             Traceback (most recent call last)\n",
    "/usr/local/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py in deco(*a, **kw)\n",
    "     62         try:\n",
    "---> 63             return f(*a, **kw)\n",
    "     64         except py4j.protocol.Py4JJavaError as e:\n",
    "\n",
    "/usr/local/anaconda3/lib/python3.7/site-packages/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)\n",
    "    327                     \"An error occurred while calling {0}{1}{2}.\\n\".\n",
    "--> 328                     format(target_id, \".\", name), value)\n",
    "    329             else:\n",
    "\n",
    "Py4JJavaError: An error occurred while calling o123.select.\n",
    ": org.apache.spark.sql.AnalysisException: cannot resolve '(CAST(`group` AS DOUBLE) + `abool`)' due to data type mismatch: differing types in '(CAST(`group` AS DOUBLE) + `abool`)' (double and boolean).;;\n",
    "'Project [(cast(group#179 as double) + abool#180) AS (group + abool)#284]\n",
    "+- Project [n#178, group#179, abool#180, n2#238, POWER(n#178, cast(2 as double)) AS n3#256]\n",
    "   +- Project [n#178, group#179, abool#180, (n#178 * cast(-1 as double)) AS n2#238]\n",
    "      +- LogicalRDD [n#178, group#179, abool#180], false\n",
    "\n",
    "\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:116)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)\n",
    "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:280)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:329)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:278)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n",
    "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n",
    "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
    "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
    "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
    "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
    "\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
    "\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n",
    "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n",
    "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n",
    "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n",
    "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n",
    "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n",
    "\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3412)\n",
    "\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1340)\n",
    "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
    "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
    "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
    "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
    "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
    "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
    "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
    "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
    "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
    "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
    "\tat java.lang.Thread.run(Thread.java:748)\n",
    "\n",
    "\n",
    "During handling of the above exception, another exception occurred:\n",
    "\n",
    "AnalysisException                         Traceback (most recent call last)\n",
    "<ipython-input-21-435d2c8b5834> in <module>\n",
    "      2 #  df.select(df.group + df.abool)\n",
    "      3 \n",
    "----> 4 dfii.select(dfii.group + dfii.abool)\n",
    "      5 \n",
    "      6 # OUTPUT: ERROR\n",
    "\n",
    "/usr/local/anaconda3/lib/python3.7/site-packages/pyspark/sql/dataframe.py in select(self, *cols)\n",
    "   1319         [Row(name=u'Alice', age=12), Row(name=u'Bob', age=15)]\n",
    "   1320         \"\"\"\n",
    "-> 1321         jdf = self._jdf.select(self._jcols(*cols))\n",
    "   1322         return DataFrame(jdf, self.sql_ctx)\n",
    "   1323 \n",
    "\n",
    "/usr/local/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py in __call__(self, *args)\n",
    "   1255         answer = self.gateway_client.send_command(command)\n",
    "   1256         return_value = get_return_value(\n",
    "-> 1257             answer, self.gateway_client, self.target_id, self.name)\n",
    "   1258 \n",
    "   1259         for temp_arg in temp_args:\n",
    "\n",
    "/usr/local/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py in deco(*a, **kw)\n",
    "     67                                              e.java_exception.getStackTrace()))\n",
    "     68             if s.startswith('org.apache.spark.sql.AnalysisException: '):\n",
    "---> 69                 raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
    "     70             if s.startswith('org.apache.spark.sql.catalyst.analysis'):\n",
    "     71                 raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
    "\n",
    "AnalysisException: \"cannot resolve '(CAST(`group` AS DOUBLE) + `abool`)' due to data type mismatch: differing types in '(CAST(`group` AS DOUBLE) + `abool`)' (double and boolean).;;\\n'Project [(cast(group#179 as double) + abool#180) AS (group + abool)#284]\\n+- Project [n#178, group#179, abool#180, n2#238, POWER(n#178, cast(2 as double)) AS n3#256]\\n   +- Project [n#178, group#179, abool#180, (n#178 * cast(-1 as double)) AS n2#238]\\n      +- LogicalRDD [n#178, group#179, abool#180], false\\n\"</code>\n",
    "\n",
    "[Return](#II.H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example II.I\n",
    "[Return](#II.I)\n",
    "\n",
    "<code>---------------------------------------------------------------------------\n",
    "Py4JJavaError                             Traceback (most recent call last)\n",
    "/usr/local/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py in deco(*a, **kw)\n",
    "     62         try:\n",
    "---> 63             return f(*a, **kw)\n",
    "     64         except py4j.protocol.Py4JJavaError as e:\n",
    "\n",
    "/usr/local/anaconda3/lib/python3.7/site-packages/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)\n",
    "    327                     \"An error occurred while calling {0}{1}{2}.\\n\".\n",
    "--> 328                     format(target_id, \".\", name), value)\n",
    "    329             else:\n",
    "\n",
    "Py4JJavaError: An error occurred while calling o123.select.\n",
    ": org.apache.spark.sql.AnalysisException: cannot resolve '(`n` + `abool`)' due to data type mismatch: differing types in '(`n` + `abool`)' (double and boolean).;;\n",
    "'Project [(n#178 + abool#180) AS (n + abool)#285]\n",
    "+- Project [n#178, group#179, abool#180, n2#238, POWER(n#178, cast(2 as double)) AS n3#256]\n",
    "   +- Project [n#178, group#179, abool#180, (n#178 * cast(-1 as double)) AS n2#238]\n",
    "      +- LogicalRDD [n#178, group#179, abool#180], false\n",
    "\n",
    "\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:116)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)\n",
    "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:280)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:329)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:278)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n",
    "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n",
    "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
    "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
    "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
    "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
    "\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
    "\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n",
    "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n",
    "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n",
    "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n",
    "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n",
    "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n",
    "\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3412)\n",
    "\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1340)\n",
    "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
    "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
    "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
    "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
    "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
    "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
    "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
    "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
    "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
    "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
    "\tat java.lang.Thread.run(Thread.java:748)\n",
    "\n",
    "\n",
    "During handling of the above exception, another exception occurred:\n",
    "\n",
    "AnalysisException                         Traceback (most recent call last)\n",
    "<ipython-input-23-fad6529b8e19> in <module>\n",
    "      1 # I Try adding various other columns together. What are the results of combining the different data types?\n",
    "      2 \n",
    "----> 3 dfii.select(dfii.n + dfii.abool).show(5)\n",
    "      4 \n",
    "      5 # OUTPUT: Error\n",
    "\n",
    "/usr/local/anaconda3/lib/python3.7/site-packages/pyspark/sql/dataframe.py in select(self, *cols)\n",
    "   1319         [Row(name=u'Alice', age=12), Row(name=u'Bob', age=15)]\n",
    "   1320         \"\"\"\n",
    "-> 1321         jdf = self._jdf.select(self._jcols(*cols))\n",
    "   1322         return DataFrame(jdf, self.sql_ctx)\n",
    "   1323 \n",
    "\n",
    "/usr/local/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py in __call__(self, *args)\n",
    "   1255         answer = self.gateway_client.send_command(command)\n",
    "   1256         return_value = get_return_value(\n",
    "-> 1257             answer, self.gateway_client, self.target_id, self.name)\n",
    "   1258 \n",
    "   1259         for temp_arg in temp_args:\n",
    "\n",
    "/usr/local/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py in deco(*a, **kw)\n",
    "     67                                              e.java_exception.getStackTrace()))\n",
    "     68             if s.startswith('org.apache.spark.sql.AnalysisException: '):\n",
    "---> 69                 raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
    "     70             if s.startswith('org.apache.spark.sql.catalyst.analysis'):\n",
    "     71                 raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
    "\n",
    "AnalysisException: \"cannot resolve '(`n` + `abool`)' due to data type mismatch: differing types in '(`n` + `abool`)' (double and boolean).;;\\n'Project [(n#178 + abool#180) AS (n + abool)#285]\\n+- Project [n#178, group#179, abool#180, n2#238, POWER(n#178, cast(2 as double)) AS n3#256]\\n   +- Project [n#178, group#179, abool#180, (n#178 * cast(-1 as double)) AS n2#238]\\n      +- LogicalRDD [n#178, group#179, abool#180], false\\n\"</code>\n",
    "\n",
    "[Return](#II.I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example III.G\n",
    "[Return](#III.G)\n",
    "\n",
    "<Code>---------------------------------------------------------------------------\n",
    "Py4JJavaError                             Traceback (most recent call last)\n",
    "/usr/local/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py in deco(*a, **kw)\n",
    "     62         try:\n",
    "---> 63             return f(*a, **kw)\n",
    "     64         except py4j.protocol.Py4JJavaError as e:\n",
    "\n",
    "/usr/local/anaconda3/lib/python3.7/site-packages/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)\n",
    "    327                     \"An error occurred while calling {0}{1}{2}.\\n\".\n",
    "--> 328                     format(target_id, \".\", name), value)\n",
    "    329             else:\n",
    "\n",
    "Py4JJavaError: An error occurred while calling o20.sql.\n",
    ": org.apache.spark.sql.AnalysisException: cannot resolve '`l`' given input columns: [my_df.n, my_df.group, my_df.abool]; line 2 pos 7;\n",
    "'Project ['l, n#284, (n#284 / cast(2 as double)) AS n2#397, (n#284 - cast(1 as double)) AS n3#398]\n",
    "+- SubqueryAlias `my_df`\n",
    "   +- LogicalRDD [n#284, group#285, abool#286], false\n",
    "\n",
    "\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)\n",
    "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:280)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n",
    "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n",
    "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
    "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
    "\tat scala.collection.immutable.List.foreach(List.scala:392)\n",
    "\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
    "\tat scala.collection.immutable.List.map(List.scala:296)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n",
    "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\n",
    "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n",
    "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n",
    "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n",
    "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n",
    "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n",
    "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n",
    "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n",
    "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\n",
    "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
    "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
    "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
    "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
    "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
    "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
    "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
    "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
    "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
    "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
    "\tat java.lang.Thread.run(Thread.java:748)\n",
    "\n",
    "\n",
    "During handling of the above exception, another exception occurred:\n",
    "\n",
    "AnalysisException                         Traceback (most recent call last)\n",
    "<ipython-input-40-0a9fc38cab0c> in <module>\n",
    "      3 spark.sql('''\n",
    "      4 SELECT l, n, (n/2) as n2, (n-1) as n3 FROM my_df\n",
    "----> 5 ''').show(5)\n",
    "\n",
    "/usr/local/anaconda3/lib/python3.7/site-packages/pyspark/sql/session.py in sql(self, sqlQuery)\n",
    "    765         [Row(f1=1, f2=u'row1'), Row(f1=2, f2=u'row2'), Row(f1=3, f2=u'row3')]\n",
    "    766         \"\"\"\n",
    "--> 767         return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n",
    "    768 \n",
    "    769     @since(2.0)\n",
    "\n",
    "/usr/local/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py in __call__(self, *args)\n",
    "   1255         answer = self.gateway_client.send_command(command)\n",
    "   1256         return_value = get_return_value(\n",
    "-> 1257             answer, self.gateway_client, self.target_id, self.name)\n",
    "   1258 \n",
    "   1259         for temp_arg in temp_args:\n",
    "\n",
    "/usr/local/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py in deco(*a, **kw)\n",
    "     67                                              e.java_exception.getStackTrace()))\n",
    "     68             if s.startswith('org.apache.spark.sql.AnalysisException: '):\n",
    "---> 69                 raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
    "     70             if s.startswith('org.apache.spark.sql.catalyst.analysis'):\n",
    "     71                 raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
    "\n",
    "AnalysisException: \"cannot resolve '`l`' given input columns: [my_df.n, my_df.group, my_df.abool]; line 2 pos 7;\\n'Project ['l, n#284, (n#284 / cast(2 as double)) AS n2#397, (n#284 - cast(1 as double)) AS n3#398]\\n+- SubqueryAlias `my_df`\\n   +- LogicalRDD [n#284, group#285, abool#286], false\\n\"</code>\n",
    "\n",
    "[Return](#III.G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
